{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ba7eb8-4fd2-4d8b-a806-c1c28eb8bc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD39 Bronze Layer: Data Ingestion in Azure Databricks  \n",
    "This section ingests raw data from CSV and JSON files into Delta format, creating **bronze tables** for customers, orders, and products.  \n",
    "These Delta tables serve as the foundation for further transformations in the Silver and Gold layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8a094e-5ec8-43a2-8c2a-dcdcbb957aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer ingestion completed.\n"
     ]
    }
   ],
   "source": [
    "# Bronze Layer - Data Ingestion (Unity Catalog Friendly)\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "delta_base = 'dbfs:/FileStore/retail360/delta'\n",
    "\n",
    "# Schemas\n",
    "customers_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('region', StringType(), True),\n",
    "    StructField('email', StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField('order_id', IntegerType(), True),\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('product', StringType(), True),\n",
    "    StructField('quantity', IntegerType(), True),\n",
    "    StructField('price', DoubleType(), True),\n",
    "    StructField('status', StringType(), True),\n",
    "    StructField('order_date', StringType(), True)\n",
    "])\n",
    "\n",
    "# --- Customers ---\n",
    "bronze_customers_path = f'{delta_base}/bronze_customers'\n",
    "customers_df = (spark.read.option('header', True)\n",
    "                .schema(customers_schema)\n",
    "                .csv('/FileStore/tables/customers-4.csv'))\n",
    "customers_df.write.format('delta').mode('overwrite').save(bronze_customers_path)\n",
    "\n",
    "# --- Orders ---\n",
    "bronze_orders_path = f'{delta_base}/bronze_orders'\n",
    "orders_df = (spark.read.option('header', True)\n",
    "             .schema(orders_schema)\n",
    "             .csv('/FileStore/tables/orders_day1-3.csv')\n",
    "             .withColumn('order_date', to_date('order_date', 'yyyy-MM-dd')))\n",
    "orders_df.write.format('delta').mode('overwrite').save(bronze_orders_path)\n",
    "\n",
    "# --- Products ---\n",
    "bronze_products_path = f'{delta_base}/bronze_products'\n",
    "products_df = spark.read.json('/FileStore/tables/products-3.json')\n",
    "products_df.write.format('delta').mode('overwrite').save(bronze_products_path)\n",
    "\n",
    "print(\"Bronze layer ingestion completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0863cd-606f-46a4-bb02-88a248503bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD38 Silver Layer: Data Cleansing & Transformation in Azure Databricks  \n",
    "This section refines the Bronze data by removing invalid records, enriching it with product and customer details, and calculating total amounts.  \n",
    "The resulting **Silver tables** contain clean, structured, and analytics-ready data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c89caf-4516-41aa-83e2-79fa0d5a6267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Silver layer transformation completed successfully.\nSilver data written to: dbfs:/FileStore/retail360/delta/silver_orders\n"
     ]
    }
   ],
   "source": [
    "# Databricks Notebook: Silver Layer - Cleansing & Transformation (Unity Catalog–Friendly)\n",
    "# Reads Bronze Delta files and produces Silver-level cleaned data.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define Delta base path\n",
    "delta_base = 'dbfs:/FileStore/retail360/delta'\n",
    "\n",
    "# --- Load Bronze Data ---\n",
    "bronze_customers = spark.read.format('delta').load(f'{delta_base}/bronze_customers')\n",
    "bronze_orders = spark.read.format('delta').load(f'{delta_base}/bronze_orders')\n",
    "bronze_products = spark.read.format('delta').load(f'{delta_base}/bronze_products')\n",
    "\n",
    "# --- Data Cleansing ---\n",
    "clean_customers = bronze_customers.filter(col('email').isNotNull())\n",
    "clean_orders = bronze_orders.filter(col('status') == 'Completed')\n",
    "\n",
    "# --- Data Enrichment ---\n",
    "# Include category in mapping\n",
    "products_map = bronze_products.selectExpr('product_id', 'product_name as product', 'category')\n",
    "\n",
    "# Join orders with customer and product data\n",
    "silver_orders = (clean_orders\n",
    "                 .join(clean_customers, on='customer_id', how='left')\n",
    "                 .join(products_map, on='product', how='left')\n",
    "                 .withColumn('total_amount', col('quantity') * col('price'))\n",
    "                 .select(\n",
    "                     'order_id', 'customer_id', 'name', 'region', 'email',\n",
    "                     'product', 'product_id', 'category',\n",
    "                     'quantity', 'price', 'total_amount',\n",
    "                     'order_date', 'status'\n",
    "                 ))\n",
    "\n",
    "# --- Write Silver Layer ---\n",
    "silver_orders_path = f'{delta_base}/silver_orders'\n",
    "silver_orders.write.format('delta').mode('overwrite').save(silver_orders_path)\n",
    "\n",
    "print(\" Silver layer transformation completed successfully.\")\n",
    "print(f\"Silver data written to: {silver_orders_path}\")\n",
    "\n",
    "# Optional preview:\n",
    "# display(spark.read.format('delta').load(silver_orders_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4d988f-60dc-4cdb-9497-2df34513f829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDFE1 Gold Layer: Business Aggregations in Azure Databricks  \n",
    "This section aggregates cleansed Silver data to generate **business insights**, including total revenue by region and top-selling products.  \n",
    "The resulting **Gold tables** provide summarized, analytics-ready datasets for reporting and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94516dd8-83b6-459b-b306-f99e86a0ab5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gold layer aggregations completed.\nGold outputs written to:\n- dbfs:/FileStore/retail360/delta/gold_revenue_by_region\n- dbfs:/FileStore/retail360/delta/gold_top_products\n- dbfs:/FileStore/retail360/delta/gold_sales_summary\n"
     ]
    }
   ],
   "source": [
    "# Databricks Notebook: Gold Layer - Business Aggregations (Unity Catalog–Friendly)\n",
    "# Reads Silver Delta data and produces Gold-level aggregated outputs.\n",
    "\n",
    "from pyspark.sql.functions import sum as _sum, rank, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define Delta paths\n",
    "delta_base = 'dbfs:/FileStore/retail360/delta'\n",
    "silver_orders_path = f'{delta_base}/silver_orders'\n",
    "gold_revenue_path = f'{delta_base}/gold_revenue_by_region'\n",
    "gold_products_path = f'{delta_base}/gold_top_products'\n",
    "gold_summary_path = f'{delta_base}/gold_sales_summary'\n",
    "\n",
    "# --- Load Silver Data ---\n",
    "silver_orders = spark.read.format('delta').load(silver_orders_path)\n",
    "\n",
    "# --- Aggregation 1: Total Revenue by Region ---\n",
    "gold_revenue_by_region = (\n",
    "    silver_orders.groupBy('region')\n",
    "    .agg(_sum('total_amount').alias('total_revenue'))\n",
    "    .orderBy(desc('total_revenue'))\n",
    ")\n",
    "\n",
    "# Save output\n",
    "gold_revenue_by_region.write.format('delta').mode('overwrite').save(gold_revenue_path)\n",
    "\n",
    "# --- Aggregation 2: Top-Selling Products ---\n",
    "product_sales = (\n",
    "    silver_orders.groupBy('product', 'product_id')\n",
    "    .agg(\n",
    "        _sum('quantity').alias('units_sold'),\n",
    "        _sum('total_amount').alias('revenue')\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Apply Ranking ---\n",
    "window_spec = Window.orderBy(desc('revenue'))\n",
    "product_sales_ranked = product_sales.withColumn('rank', rank().over(window_spec))\n",
    "\n",
    "# Save output\n",
    "product_sales_ranked.write.format('delta').mode('overwrite').save(gold_products_path)\n",
    "\n",
    "# --- Combined Gold Summary ---\n",
    "gold_summary = product_sales_ranked.select('product', 'product_id', 'units_sold', 'revenue', 'rank')\n",
    "gold_summary.write.format('delta').mode('overwrite').save(gold_summary_path)\n",
    "\n",
    "print(\" Gold layer aggregations completed.\")\n",
    "print(f\"Gold outputs written to:\\n- {gold_revenue_path}\\n- {gold_products_path}\\n- {gold_summary_path}\")\n",
    "\n",
    "# Optional preview:\n",
    "# display(spark.read.format('delta').load(gold_summary_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce91ebef-fe57-4dfe-8b69-ddfebdcd6c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD39 Incremental Load Simulation: Updating Silver Layer in Azure Databricks  \n",
    "This notebook simulates new incoming orders and **merges** them into the existing Silver Layer.  \n",
    "Steps include loading new data, appending to Bronze, staging transformed records, and performing a **MERGE/UPSERT** to keep the Silver Layer up-to-date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22df7cb6-a8b6-480d-9bec-ab0aef37dc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New incremental orders loaded.\n Appended new orders to Bronze layer.\n Staging Delta file created for new orders.\n Incremental MERGE completed. Silver layer updated with new data.\n"
     ]
    }
   ],
   "source": [
    "# Databricks Notebook: Incremental Load Simulation (Unity Catalog–Friendly)\n",
    "# Purpose: Simulate new incoming orders and merge them into the Silver Layer.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define base paths\n",
    "delta_base = 'dbfs:/FileStore/retail360/delta'\n",
    "silver_orders_path = f'{delta_base}/silver_orders'\n",
    "bronze_orders_path = f'{delta_base}/bronze_orders'\n",
    "\n",
    "# --- Schema for new order data ---\n",
    "orders_schema = StructType([\n",
    "    StructField('order_id', IntegerType(), True),\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('product', StringType(), True),\n",
    "    StructField('quantity', IntegerType(), True),\n",
    "    StructField('price', DoubleType(), True),\n",
    "    StructField('status', StringType(), True),\n",
    "    StructField('order_date', StringType(), True)\n",
    "])\n",
    "\n",
    "# ---  Read New Incremental Orders File ---\n",
    "# Make sure you upload your new file, for example:\n",
    "# /FileStore/tables/orders_day2.csv\n",
    "new_orders = (spark.read\n",
    "              .option('header', True)\n",
    "              .schema(orders_schema)\n",
    "              .csv('/FileStore/tables/orders_day2.csv')\n",
    "              .withColumn('order_date', to_date('order_date', 'yyyy-MM-dd')))\n",
    "\n",
    "print(\" New incremental orders loaded.\")\n",
    "\n",
    "# ---  Append to Bronze Orders (simulated raw ingestion) ---\n",
    "new_orders.write.format('delta').mode('append').save(bronze_orders_path)\n",
    "print(\" Appended new orders to Bronze layer.\")\n",
    "\n",
    "# ---  Rebuild Silver Staging from new orders ---\n",
    "# Reload reference data\n",
    "bronze_customers = spark.read.format('delta').load(f'{delta_base}/bronze_customers')\n",
    "bronze_products = spark.read.format('delta').load(f'{delta_base}/bronze_products')\n",
    "\n",
    "clean_customers = bronze_customers.filter(col('email').isNotNull())\n",
    "products_map = bronze_products.selectExpr('product_id', 'product_name as product', 'category')\n",
    "\n",
    "staging_new = (new_orders\n",
    "               .filter(col('status') == 'Completed')\n",
    "               .join(clean_customers, on='customer_id', how='left')\n",
    "               .join(products_map, on='product', how='left')\n",
    "               .withColumn('total_amount', col('quantity') * col('price'))\n",
    "               .select(\n",
    "                   'order_id', 'customer_id', 'name', 'region', 'email',\n",
    "                   'product', 'product_id', 'category',\n",
    "                   'quantity', 'price', 'total_amount',\n",
    "                   'order_date', 'status'\n",
    "               ))\n",
    "\n",
    "staging_path = f'{delta_base}/staging_new_orders'\n",
    "staging_new.write.format('delta').mode('overwrite').save(staging_path)\n",
    "print(\" Staging Delta file created for new orders.\")\n",
    "\n",
    "# ---  MERGE / UPSERT into Silver Orders ---\n",
    "silver_table = DeltaTable.forPath(spark, silver_orders_path)\n",
    "staging_table = spark.read.format('delta').load(staging_path)\n",
    "\n",
    "(silver_table.alias('t')\n",
    " .merge(\n",
    "     staging_table.alias('s'),\n",
    "     't.order_id = s.order_id'\n",
    " )\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute())\n",
    "\n",
    "print(\" Incremental MERGE completed. Silver layer updated with new data.\")\n",
    "\n",
    "# Optional: Verify\n",
    "# display(spark.read.format('delta').load(silver_orders_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d6de260-4988-40c5-9c7d-9e85033cbc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD52 Time Travel & Vacuum in Azure Databricks  \n",
    "This notebook demonstrates **Delta Lake time travel** to explore historical versions of Gold tables and uses **VACUUM** to clean up old, unneeded data files while maintaining retention policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572638bc-9e27-4e2c-ada6-4d8411e7aeee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Delta table history for gold_sales_summary:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>3</td><td>2025-10-13T08:10:32.000Z</td><td>146222421029543</td><td>azuser4803_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3458400548074996)</td><td>1013-050721-4duilhgc-v2n</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 1518, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1518)</td><td>null</td><td>Databricks-Runtime/17.2.x-photon-scala2.13</td></tr><tr><td>2</td><td>2025-10-13T07:56:54.001Z</td><td>146222421029543</td><td>azuser4803_mml.local@techademy.com</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(3458400548074996)</td><td>1013-050721-4duilhgc-v2n</td><td>1</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 0, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/17.2.x-photon-scala2.13</td></tr><tr><td>1</td><td>2025-10-13T07:56:54.000Z</td><td>146222421029543</td><td>azuser4803_mml.local@techademy.com</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000)</td><td>null</td><td>List(3458400548074996)</td><td>1013-050721-4duilhgc-v2n</td><td>0</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 0, sizeOfDataToDelete -> 0)</td><td>null</td><td>Databricks-Runtime/17.2.x-photon-scala2.13</td></tr><tr><td>0</td><td>2025-10-13T07:46:19.000Z</td><td>146222421029543</td><td>azuser4803_mml.local@techademy.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3458400548074996)</td><td>1013-050721-4duilhgc-v2n</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numDeletionVectorsRemoved -> 0, numOutputRows -> 3, numOutputBytes -> 1518)</td><td>null</td><td>Databricks-Runtime/17.2.x-photon-scala2.13</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "2025-10-13T08:10:32.000Z",
         "146222421029543",
         "azuser4803_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3458400548074996"
         ],
         "1013-050721-4duilhgc-v2n",
         2,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "1",
          "numOutputBytes": "1518",
          "numOutputRows": "3",
          "numRemovedBytes": "1518",
          "numRemovedFiles": "1"
         },
         null,
         "Databricks-Runtime/17.2.x-photon-scala2.13"
        ],
        [
         2,
         "2025-10-13T07:56:54.001Z",
         "146222421029543",
         "azuser4803_mml.local@techademy.com",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "3458400548074996"
         ],
         "1013-050721-4duilhgc-v2n",
         1,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "0",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/17.2.x-photon-scala2.13"
        ],
        [
         1,
         "2025-10-13T07:56:54.000Z",
         "146222421029543",
         "azuser4803_mml.local@techademy.com",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "true",
          "specifiedRetentionMillis": "604800000"
         },
         null,
         [
          "3458400548074996"
         ],
         "1013-050721-4duilhgc-v2n",
         0,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "0",
          "sizeOfDataToDelete": "0"
         },
         null,
         "Databricks-Runtime/17.2.x-photon-scala2.13"
        ],
        [
         0,
         "2025-10-13T07:46:19.000Z",
         "146222421029543",
         "azuser4803_mml.local@techademy.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3458400548074996"
         ],
         "1013-050721-4duilhgc-v2n",
         null,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "1",
          "numOutputBytes": "1518",
          "numOutputRows": "3",
          "numRemovedBytes": "0",
          "numRemovedFiles": "0"
         },
         null,
         "Databricks-Runtime/17.2.x-photon-scala2.13"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"jobId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobRunId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"runId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobOwnerId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"triggerType\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"notebookId\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully loaded version 0 of gold_sales_summary.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product</th><th>product_id</th><th>units_sold</th><th>revenue</th><th>rank</th></tr></thead><tbody><tr><td>Laptop</td><td>P001</td><td>2</td><td>110000.0</td><td>1</td></tr><tr><td>Mobile</td><td>P002</td><td>3</td><td>75000.0</td><td>2</td></tr><tr><td>Headphones</td><td>P004</td><td>5</td><td>15000.0</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Laptop",
         "P001",
         2,
         110000.0,
         1
        ],
        [
         "Mobile",
         "P002",
         3,
         75000.0,
         2
        ],
        [
         "Headphones",
         "P004",
         5,
         15000.0,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "units_sold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "rank",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VACUUM completed successfully (7-day retention).\n\uD83C\uDFC1 Time Travel & Vacuum demo complete.\n"
     ]
    }
   ],
   "source": [
    "# Databricks Notebook: Time Travel & Vacuum (Unity Catalog–Friendly)\n",
    "# Purpose: Explore historical Delta versions and clean up old data files.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define paths\n",
    "delta_base = 'dbfs:/FileStore/retail360/delta'\n",
    "gold_summary_path = f'{delta_base}/gold_sales_summary'\n",
    "\n",
    "# ---  View Delta Table History ---\n",
    "print(\" Delta table history for gold_sales_summary:\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{gold_summary_path}`\")\n",
    "display(history_df)\n",
    "\n",
    "# Optional: To check programmatically\n",
    "# history_df.show(truncate=False)\n",
    "\n",
    "# ---  Time Travel: Read a Previous Version ---\n",
    "# Example: Load version 0 (or whichever version you want to inspect)\n",
    "try:\n",
    "    old_version_df = (spark.read\n",
    "                      .format('delta')\n",
    "                      .option('versionAsOf', 0)\n",
    "                      .load(gold_summary_path))\n",
    "    print(\" Successfully loaded version 0 of gold_sales_summary.\")\n",
    "    display(old_version_df)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not load version 0. Maybe that version doesn’t exist yet.\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    gold_dt = DeltaTable.forPath(spark, gold_summary_path)\n",
    "    gold_dt.vacuum(retentionHours=168)  # Keep last 7 days\n",
    "    print(\" VACUUM completed successfully (7-day retention).\")\n",
    "except Exception as e:\n",
    "    print(\" VACUUM failed — check cluster permissions or retention settings.\")\n",
    "    print(e)\n",
    "\n",
    "print(\" Time Travel & Vacuum demo complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail Data Analytics Platform using Azure Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}