{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRB9dp12YqQR",
        "outputId": "cfd681bb-7504-45d2-d91d-215c5b89a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Setup Java Environment (often needed in Colab)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg, month, year\n",
        "\n",
        "# 1. Initialize Spark Session and Load Data\n",
        "# --------------------------------------------------------------------------------\n",
        "# Create the Spark Session\n",
        "spark = SparkSession.builder.appName(\"RetailSalesInsights\").getOrCreate()\n",
        "\n",
        "# Load the cleaned CSV file from Week 2\n",
        "# NOTE: File name must match the uploaded file.\n",
        "sales_df = spark.read.csv(\"cleaned_sales_with_metrics.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Ensure sale_date is treated as a date for monthly grouping\n",
        "sales_df = sales_df.withColumn(\"sale_date\", col(\"sale_date\").cast(\"date\"))\n",
        "sales_df = sales_df.withColumn(\"sale_month\", month(col(\"sale_date\")))\n",
        "sales_df = sales_df.withColumn(\"sale_year\", year(col(\"sale_date\")))\n",
        "\n",
        "\n",
        "# 2. Filter for Underperforming Products (Example Logic)\n",
        "# --------------------------------------------------------------------------------\n",
        "# Identify products with low revenue (e.g., less than $1000) AND negative margin\n",
        "\n",
        "product_summary = sales_df.groupBy(\"product_name\").agg(\n",
        "    sum(\"revenue\").alias(\"Total_Revenue\"),\n",
        "    avg(\"profit_margin\").alias(\"Avg_Profit_Margin\")\n",
        ")\n",
        "\n",
        "# Filter for underperformers\n",
        "underperforming_list = product_summary.filter(\n",
        "    (col(\"Total_Revenue\") < 1000) & (col(\"Avg_Profit_Margin\") < 0)\n",
        ")\n",
        "underperforming_list.show(truncate=False)\n",
        "\n",
        "\n",
        "# 3. Group by Store and Calculate Average Monthly Revenue\n",
        "# --------------------------------------------------------------------------------\n",
        "# Calculate total monthly revenue per store\n",
        "monthly_revenue = sales_df.groupBy(\"store_id\", \"sale_year\", \"sale_month\").agg(\n",
        "    sum(\"revenue\").alias(\"Monthly_Revenue_Total\")\n",
        ")\n",
        "\n",
        "# Calculate the average of the monthly revenue totals per store\n",
        "store_avg_monthly_revenue = monthly_revenue.groupBy(\"store_id\").agg(\n",
        "    avg(\"Monthly_Revenue_Total\").alias(\"Average_Monthly_Revenue\")\n",
        ").sort(col(\"Average_Monthly_Revenue\").desc())\n",
        "\n",
        "store_avg_monthly_revenue.show()\n",
        "\n",
        "\n",
        "# 4. Deliverables: Save Output Files\n",
        "# --------------------------------------------------------------------------------\n",
        "# PySpark writes files into folders, not single CSVs.\n",
        "\n",
        "# Save the PySpark script output for store summary (Week 3 Deliverable)\n",
        "store_avg_monthly_revenue.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"pyspark_store_summary\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# Save the underperforming products list\n",
        "underperforming_list.coalesce(1).write.mode(\"overwrite\").csv(\n",
        "    \"pyspark_underperforming_products\",\n",
        "    header=True\n",
        ")\n",
        "\n",
        "# Stop the Spark session (Good practice in Colab)\n",
        "spark.stop()\n",
        "\n",
        "print(\"\\nWEEK 3 COMPLETE. Output files are saved in the Colab file explorer under the 'pyspark_store_summary/' and 'pyspark_underperforming_products/' folders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCJbq0mCbCYI",
        "outputId": "30370769-d026-432c-eccf-698816f8a847"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+-----------------+\n",
            "|product_name|Total_Revenue|Avg_Profit_Margin|\n",
            "+------------+-------------+-----------------+\n",
            "+------------+-------------+-----------------+\n",
            "\n",
            "+--------+-----------------------+\n",
            "|store_id|Average_Monthly_Revenue|\n",
            "+--------+-----------------------+\n",
            "|    1002|                 2080.0|\n",
            "|    1001|                1261.25|\n",
            "+--------+-----------------------+\n",
            "\n",
            "\n",
            "WEEK 3 COMPLETE. Output files are saved in the Colab file explorer under the 'pyspark_store_summary/' and 'pyspark_underperforming_products/' folders.\n"
          ]
        }
      ]
    }
  ]
}